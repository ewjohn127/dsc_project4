{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](dogecoin.jpg \"Dogecoin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dogecoin](images/dogecoin.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hedgefund is seeking to profit from the volatility and inefficiences of the cryptocurrency market. They would like to focus on Dogecoin, a cryptocurrency with a $30B market cap. Their marketing team desires to better understand the sentiment around dogecoin, for understanding other owners and what words or phrases most reseonate with them. \n",
    "\n",
    "Dogecoin began as satire, using a meme of a Shiba Inu dog misspelled as \"doge.\" The makers, Billy Marcus and Jackson Palmer, created it in late 2013. Then launched a large campaing including sending the Jamaican Bobled team to the '14 olympics and sponsoring a Nascar. \n",
    "\n",
    "As a cryptocurrency, Dogecoin also relies on block-chain cryptography. All coin holder's carry a ledger. And miner's have to solve mathproblems to create new chains. They are rewarded with dogecoin. \n",
    "\n",
    "Dogecoin differs from many other cryptocurrency in that it has no lifetime cap on the number of coins that can be produced. This means it is highly inflationary--by design. This diminishes the coin as a store of value, but increases its use as an actual currency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Market volatililty and the speculative nature of Dogecoin increase risk of owning asset. Since the price of other cryptocurrencies have proven susceptible to the sentiment of its buyers and sellers (citation), the hedfund seeks deeper comprehension of the sentiment of those very buyers and seller, in order to get a better grasp on the market, and possibly leverage that information for profits in the future. This project uses Twitter as a proxy for the buyers and sellers, and analyzes tweets containing the word \"dogecoin.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter API, Tweets, Features, Target\n",
    "\n",
    "Twitter's API has limitations on retrieval. One can only pull tweets from the last 7days, in increments of 180 tweets every 15 minuts, with a cap of 2,600 at one time. Thus, it takes 3 and 1/2 hours to pull accumulate 2,500 tweets. \n",
    "\n",
    "Retweets will be excluded because the Hedgefund is trying to get a sense of dogecoin audience, not the retweets of one bot (which we discovered within our first few pulls)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API Request Code\n",
    "```\n",
    "# Authentication\n",
    "consumerKey = creds['API Key']\n",
    "consumerSecret = creds['API Key Secret']\n",
    "accessToken = creds['Access Token']\n",
    "accessTokenSecret = creds['Access Token Secret']\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "#Pull tweets based on Keyword and Amount (noOfTweet)\n",
    "keyword = input('Please enter keyword or hastag to search: ')\n",
    "noOfTweet = int(input ('Please enter how many tweets to analyze: '))\n",
    "tweets = tweepy.Cursor(api.search_tweets, q=keyword+' -filter:retweets').items(noOfTweet)\n",
    "tweet_list = []\n",
    "tweet_date_list = []\n",
    "for tweet in tweets:\n",
    "    tweet_list.append(tweet.text)\n",
    "    tweet_date_list.append(tweet.created_at)\n",
    "\n",
    "tweet_list = pd.DataFrame(tweet_list)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate Tweets\n",
    "Because Twitter's API has a limit, we had to pull tweets in batches. Below, we concatenate all these serparate pulls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@binance @BinanceChain @dogecoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@DogecoinNorway @occupymars42069 @dogeofficial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16 November 2021, 07:36h \\r\\n\\r\\nThe current p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Give some tip today to a friend or family memb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Jayecane Help everyone by getting them into #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>RT @SahariCharity: DYOR @racoontoken_  #Racoon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>RT @TravisNTravesty: @Outerspacefris1 This is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Amk bulastik bi coin isine ipnenin zevkini bek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>@Vrom14286662 @bitcoin43 @Bitcoin @litecoin @d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>@papacthulu @MattWallace888 @PabzSantos30 @kid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0                     @binance @BinanceChain @dogecoin\n",
       "1    @DogecoinNorway @occupymars42069 @dogeofficial...\n",
       "2    16 November 2021, 07:36h \\r\\n\\r\\nThe current p...\n",
       "3    Give some tip today to a friend or family memb...\n",
       "4    @Jayecane Help everyone by getting them into #...\n",
       "..                                                 ...\n",
       "994  RT @SahariCharity: DYOR @racoontoken_  #Racoon...\n",
       "996  RT @TravisNTravesty: @Outerspacefris1 This is ...\n",
       "997  Amk bulastik bi coin isine ipnenin zevkini bek...\n",
       "998  @Vrom14286662 @bitcoin43 @Bitcoin @litecoin @d...\n",
       "999  @papacthulu @MattWallace888 @PabzSantos30 @kid...\n",
       "\n",
       "[3100 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.DataFrame(pd.read_csv('data/doge_tweets_111621_1138',index_col=0)['text'])\n",
    "df_2 = pd.DataFrame(pd.read_csv('data/dogecoin2_11_16_21_1pm.csv',index_col=0)['text'])\n",
    "df_3 = pd.DataFrame(pd.read_csv('data/dogecoin_11_17_21_10am.csv', index_col=0)['text'])\n",
    "df=pd.concat([df_1,df_2, df_3])\n",
    "df.drop_duplicates(inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function for vectorizing\n",
    "Note, this function removes stopwords and sorts the vectorized  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removed stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_gram(corpus,ngram_range,n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range, stop_words = 'english',max_df=1.0, min_df=0.01).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n], bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_gram_test(corpus, trained_corpus, ngram_range,n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range, stop_words = 'english',max_df=1.0, min_df=0.01).fit(trained_corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n], bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Tweets\n",
    "We removed punctuation, in order to track similar words. Likewise, we made every word lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove RT\n",
    "remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n",
    "# Remove punctuation\n",
    "rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+://\\S+)\",\" \",x)\n",
    "# Remove Retweets\n",
    "df[\"clean_text\"] = df.text.map(remove_rt).map(rt)\n",
    "# Lower the case \n",
    "df[\"clean_text\"] = df.clean_text.str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.309028</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.445455</td>\n",
       "      <td>0.351515</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity  subjectivity  sentiment\n",
       "0  0.000000      0.000000        0.0\n",
       "1  0.000000      0.000000        0.0\n",
       "2  0.000000      0.350000        0.0\n",
       "3  0.000000      0.000000        0.0\n",
       "4  0.000000      0.000000        0.0\n",
       "5  0.000000      0.000000        0.0\n",
       "6  0.309028      0.729167        0.0\n",
       "7  0.445455      0.351515        0.0\n",
       "8 -0.400000      0.400000        0.0\n",
       "9  0.000000      0.000000        0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating Negative, Positive, Neutral and Compound values\n",
    "df_probs = pd.DataFrame()\n",
    "df_probs[['polarity', 'subjectivity']] = df['clean_text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in df['text'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if neg > pos:\n",
    "        df_probs.loc[index, 'sentiment'] = 0\n",
    "    elif pos > neg:\n",
    "        df_probs.loc[index, 'sentiment'] = 1\n",
    "    else:\n",
    "        df_probs.loc[index, 'sentiment'] = 0\n",
    "df_probs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@binance @BinanceChain @dogecoin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@DogecoinNorway @occupymars42069 @dogeofficial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16 November 2021, 07:36h \\r\\n\\r\\nThe current p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Give some tip today to a friend or family memb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Jayecane Help everyone by getting them into #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3095</th>\n",
       "      <td>RT @SahariCharity: DYOR @racoontoken_  #Racoon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3096</th>\n",
       "      <td>RT @TravisNTravesty: @Outerspacefris1 This is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3097</th>\n",
       "      <td>Amk bulastik bi coin isine ipnenin zevkini bek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>@Vrom14286662 @bitcoin43 @Bitcoin @litecoin @d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>@papacthulu @MattWallace888 @PabzSantos30 @kid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text\n",
       "0                      @binance @BinanceChain @dogecoin\n",
       "1     @DogecoinNorway @occupymars42069 @dogeofficial...\n",
       "2     16 November 2021, 07:36h \\r\\n\\r\\nThe current p...\n",
       "3     Give some tip today to a friend or family memb...\n",
       "4     @Jayecane Help everyone by getting them into #...\n",
       "...                                                 ...\n",
       "3095  RT @SahariCharity: DYOR @racoontoken_  #Racoon...\n",
       "3096  RT @TravisNTravesty: @Outerspacefris1 This is ...\n",
       "3097  Amk bulastik bi coin isine ipnenin zevkini bek...\n",
       "3098  @Vrom14286662 @bitcoin43 @Bitcoin @litecoin @d...\n",
       "3099  @papacthulu @MattWallace888 @PabzSantos30 @kid...\n",
       "\n",
       "[3100 rows x 1 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df_pos = df[['text']]\n",
    "df_probs.reset_index(drop=True, inplace=True)\n",
    "df_pos['sentiment'] = df_probs['sentiment']\n",
    "df_pos = df_pos[df_pos['sentiment'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-80-f308b8778706>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pos[\"clean_text\"] = df_pos.text.map(remove_rt).map(rt).copy(deep=True)\n",
      "<ipython-input-80-f308b8778706>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pos[\"clean_text\"] = df_pos.clean_text.str.lower().copy(deep=True)\n"
     ]
    }
   ],
   "source": [
    "# Remove RT\n",
    "remove_rt = lambda x: re.sub('RT @\\w+: ',\" \",x)\n",
    "# Remove punctuation\n",
    "rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+://\\S+)\",\" \",x)\n",
    "# Remove Retweets\n",
    "df_pos[\"clean_text\"] = df_pos.text.map(remove_rt).map(rt).copy(deep=True)\n",
    "# Lower the case \n",
    "df_pos[\"clean_text\"] = df_pos.clean_text.str.lower().copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dogecoin', 'project', 'doge', 'new', 'earn', 'friend', 'income', 'pasive', 'day', '10', 'support', 'twitter', 'dogecoins', 'a6n4rywnuahiy9nncneozajbgc2opmkqmo', 'facebook', 'instagram', 'gift', 'socialmedia', 'bitcoin', 'whale', 'pinterest', 'ba', 'crypto', 'swap', 'alert', 'tx', 'gifts', 'deal', 'good', 'like', 'price', 'catecoin', 'linkedin', 'current', 'tumblr', 'btc', 'shiba', 'update', 'great', 'token', 'don', 'eth', 'io', 'moon', 'shib', 'just', 'ethereum', 'ready', 'dogearmy', 'future', 'buy', 'birthdaygirl', 'affiliatemarketing', 'worth', 'free', 'amp', 'value', 'usd', 'happy', 'nft', 'empire', 'add', 'addr', 'change', 'make', 'amazing', 'dep', 'awesome', 'cryptocurrency', 'shibainu', 'best', 'legacy', 'says', 'minute', 'hour', 'important', 'community', 'love', 'inu', 'coming', 'ev', 'tippingtuesday', 'join', 'ad', 'global', 'let', 'mdoge', 'social', 'coins', 'devs', 'accept', 'market', 'better', 'birthday', 'cash', 'think', 'time', 'binance', 'looking', 'miss']\n"
     ]
    }
   ],
   "source": [
    "positives, pos_vec = get_top_n_gram(df_pos['clean_text'], (1,1), 100)\n",
    "positives = [X[0] for X in positives]\n",
    "print(positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andy.schmeck.OFFICE\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\andy.schmeck.OFFICE\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def extract_features(text, top_100):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    features = dict()\n",
    "    wordcount = 0\n",
    "    compound_scores = list()\n",
    "    positive_scores = list()\n",
    "\n",
    "    for sentence in sent_tokenize(text):\n",
    "        for word in word_tokenize(sentence):\n",
    "            if word.lower() in top_100:\n",
    "                wordcount += 1\n",
    "        compound_scores.append(sia.polarity_scores(sentence)[\"compound\"])\n",
    "        positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
    "\n",
    "    # Adding 1 to the final compound score to always have positive numbers\n",
    "    # since some classifiers you'll use later don't work with negative numbers.\n",
    "    features[\"mean_compound\"] = np.mean(compound_scores) + 1\n",
    "    features[\"mean_positive\"] = np.mean(positive_scores)\n",
    "    features[\"wordcount\"] = wordcount\n",
    "\n",
    "    return features['wordcount']\n",
    "\n",
    "features = [\n",
    "    (extract_features(review, positives))\n",
    "    for review in df['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                      \n",
       "1                           oh  dogecoin i luv you     \n",
       "2     16 november 2021  07 36h     the current price...\n",
       "3     give some tip today to a friend or family memb...\n",
       "4       help everyone by getting them into  dogecoin...\n",
       "5        messi  binance  bitcoin  eysevgili  sezaika...\n",
       "6       i agree  the original dogecoin community was...\n",
       "7       empire haha i don t even have money to buy a...\n",
       "8                 once  shibarium launches  it s gam...\n",
       "9      webtalk  bitcoin  tumblr  twitter  facebook  ...\n",
       "10     linkedin  twitter  facebook  instagram  tikto...\n",
       "11     entrepreneurs  selfemployed  traveling  affil...\n",
       "12     linkedin  twitter  facebook  instagram  bitco...\n",
       "13     tumblr  twitter  facebook  instagram  dogecoi...\n",
       "14     linkedin  twitter  facebook  instagram  dogec...\n",
       "15     businessman  tumblr  twitter  facebook  insta...\n",
       "16     businessman  tumblr  twitter  facebook  insta...\n",
       "17     businessman  blogs  blogger  tumblr  twitter ...\n",
       "18    31  off     lg 48  class 4k uhd smart tv w ai ...\n",
       "19           baby dogecoin       babydogecoin  babyd...\n",
       "20              dogecoin  doge  dogearmy  dogefam      \n",
       "21                        dogecoin holders be like     \n",
       "22                                  empire fake acct   \n",
       "23                kazlar hakkinda son gel  meler ve ...\n",
       "24                it truly makes doge a unique meme ...\n",
       "25     dogecoin  update the current price of one dog...\n",
       "26     spidermannowayhome  sandman  andrewgarfield  ...\n",
       "27     dogecoin is not a trend      it s lifestyle  ...\n",
       "28    bitcoin price  usd   60485 25   ethereum price...\n",
       "29    elon s bastard  damn coin  it s been down for ...\n",
       "30    congratulations to   and the  safemoon team   ...\n",
       "31            empire current dips mean more are coming \n",
       "32      what sucks is that new ones pop up damn near...\n",
       "33    meme  coins have been all the rave recently wi...\n",
       "34                dogefam  dogecoin  teachmehowtodoge  \n",
       "35      daily       dogebreakfast         best game ...\n",
       "36      hype it up to make a bigger profit   i will ...\n",
       "37        empire mission to the poorhouse while some...\n",
       "38     dogecoin if you could go ahead and moon  yeah...\n",
       "39    this makes me so bullish   who has the more st...\n",
       "40        he got lucky with  dogecoin i don t think ...\n",
       "41      yes  chainlink  cardano  dogecoin   holochai...\n",
       "42       dogefam  dogearmy  dogecoin  doge loves us ...\n",
       "43     businessman  tumblr  twitter  facebook  insta...\n",
       "44     businessman  tumblr  twitter  facebook  insta...\n",
       "45     businessman  blogs  blogger  tumblr  twitter ...\n",
       "46     makemoneyonline  affiliatemarketing  networkm...\n",
       "47     linkedin  twitter  facebook  instagram  tikto...\n",
       "48     linkedin  twitter  facebook  instagram  bitco...\n",
       "49     tumblr  twitter  facebook  instagram  dogecoi...\n",
       "Name: clean_text, dtype: object"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat = pd.concat([df['clean_text'], pd.DataFrame(features)], axis=1)\n",
    "df_feat['clean_text'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>babydogepaid airdrops    joining get 100 00...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>dogecoin whale alert    tx  26...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>linkedin  twitter  facebook  instagram  tikto...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>let s look at the top 3 meme based gainers of ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>of bsc      catecoin      the king of meme i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>current  doge price is  0 23776  dogecoin  cry...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>not so mr  wonderful says  dogecoin holders di...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>dilshad   ba swap my friend  there is a new ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>saitama shiba i ll have all the ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>dogecoin  doge</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2325 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clean_text   0\n",
       "2803     babydogepaid airdrops    joining get 100 00...   4\n",
       "866                   dogecoin whale alert    tx  26...   5\n",
       "674    linkedin  twitter  facebook  instagram  tikto...  10\n",
       "468   let s look at the top 3 meme based gainers of ...   2\n",
       "1299    of bsc      catecoin      the king of meme i...   2\n",
       "...                                                 ...  ..\n",
       "2763  current  doge price is  0 23776  dogecoin  cry...   5\n",
       "905   not so mr  wonderful says  dogecoin holders di...   2\n",
       "1096    dilshad   ba swap my friend  there is a new ...  10\n",
       "235               saitama shiba i ll have all the ca...   1\n",
       "1061                                   dogecoin  doge     2\n",
       "\n",
       "[2325 rows x 2 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train-test split\n",
    "X_train,X_test,y_train,y_test = train_test_split(df_feat, df_probs['sentiment'],random_state=1)\n",
    "# top_list,vec = get_top_n_gram(X_train['text'], (2,3),500)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat, feat_vec_train = get_top_n_gram(X_train['clean_text'], (1,1), 100)\n",
    "feat, feat_vec_test = get_top_n_gram_test(X_test['clean_text'], X_train['clean_text'], (1,1), 100)\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train_vec = pd.concat([X_train[0], pd.DataFrame(feat_vec_train.todense())], axis=1)\n",
    "X_test_vec = pd.concat([X_test[0], pd.DataFrame(feat_vec_test.todense())], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2321</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2322</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2323</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2324</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2325 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16\n",
       "0      4   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "1      5   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0\n",
       "2     10   0   0   0   0   0   1   1   1   1   1   1   0   1   0   1   0   1\n",
       "3      2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "4      2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "...   ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
       "2320   5   0   0   1   0   1   1   0   0   0   0   0   0   0   0   0   0   0\n",
       "2321   2   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0   0   0\n",
       "2322  10   0   0   0   0   0   1   0   0   0   0   0   1   0   1   0   0   0\n",
       "2323   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
       "2324   2   0   0   0   0   1   1   0   0   0   0   0   0   0   0   0   0   0\n",
       "\n",
       "[2325 rows x 18 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline is the average of the majority class, which is 68% neutral/negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3100, 21137)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1- y_train.sum()/y_train.shape[0])*100\n",
    "\n",
    "vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72258065 0.68172043 0.71827957 0.67311828 0.70322581]\n",
      "predictions\n",
      "0.0    556\n",
      "1.0    219\n",
      "dtype: int64\n",
      "Accuracy Score: 0.7264516129032258\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_vec,y_train)\n",
    "\n",
    "#Cross-Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print(cross_val_score(nb, X_train_vec, y_train))\n",
    "\n",
    "# Predictions\n",
    "y_preds = nb.predict(X_test_vec)\n",
    "df_fin = pd.DataFrame()\n",
    "df_fin['predictions'] = y_preds\n",
    "print(df_fin.value_counts('predictions'))\n",
    "\n",
    "# Metric\n",
    "print('Accuracy Score:',accuracy_score(y_test,df_fin['predictions']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x19e8b7a9fd0>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(nb, X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend using these words and phrases. \n",
    "\n",
    "They mean this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going forward, we would love to offer sentiment analysis over time, along with the quantity of tweets at any given time, to predict price changes. \n",
    "\n",
    "We also would love to anaylze different excahnges. for example, reddit's wall street bets often trades in Robinhood. So doing a comparative sentiment analysis might help with arbitrage of dogecoin or between other currencies. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7a3d059f376a9d0551670ac739dcc834dd342b8d7d90019c6bdbef463e084516"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('learn-env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
