{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install Libraries\n",
    "# !pip install textblob\n",
    "# !pip install tweepy\n",
    "# !pip install pycountry\n",
    "# !pip install wordcloud\n",
    "# !pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'consumerKey': '8d2USPEgjiQt7q1CGfNKOtC9I',\n",
       " 'consumerSecret': '7vBoE8xG7f1I4XTQdnwI2U40JGh7oz4wQVuZwf02I1x4Q8N2yK',\n",
       " 'accessToken': '1324724489888976896-TCPOGt5Cfx5bGRV3u5y642E6WEjFoW',\n",
       " 'accessTokenSecret': 'fI099EqG4B9Jej9kYqSvqPfUjV2IeflEJBgRC4CTI5WMC'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/Users/andy.schmeck.OFFICE/Documents/Flatiron/phase_4/project_4/dsc_project4/.secret/twitter_api.json') as f:\n",
    "    creds = json.load(f)\n",
    "creds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication\n",
    "consumerKey = creds['consumerKey']\n",
    "consumerSecret = creds['consumerSecret']\n",
    "accessToken = creds['accessToken']\n",
    "accessTokenSecret = creds['accessTokenSecret']\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @ShirtBimbo: https://t.co/k1sLlldu8U\n",
      "And this\n",
      "https://t.co/mENFIDCoPd\n",
      "How do people buy decent mattresses these days? Am I supposed to get one of those interweb brands mailed to me?\n",
      "\n",
      "Ma‚Ä¶ https://t.co/JXcKUp9DiX\n",
      "@ozziegooen @PTetlock Thanks for pointing this out.\n",
      "my neighbors have no idea what my job is, which is üëåüèªperfect. I‚Äôm on stop-by-for-coffee terms with most of my neigh‚Ä¶ https://t.co/K5LBWAfCyp\n",
      "Man. People can read all sorts of things you don‚Äôt say into what you write https://t.co/8BBoBsVVS4\n",
      "If the bitcoin thesis is correct, then bitcoin advocates need to expend exactly zero effort arguing against other c‚Ä¶ https://t.co/zeHm93ztE6\n",
      "@DavidDeutschOxf The poll got 198 responses, from a (presumably) tiny, very particular corner of academia (probably‚Ä¶ https://t.co/EOPlAj9AvK\n",
      "RT @LozzaFox: ‚ÄúVaccine passports will absolutely end human liberty in the west.‚Äù https://t.co/j7pDK1lV3B\n",
      "@normonics Same. \n",
      "\n",
      "Good explanation of this in Beginning of Infinity. \n",
      "\n",
      "Not sure if you‚Äôve heard of that book.\n",
      "RT @paulkrugman: Trying to clarify my own thoughts on inflation. I got inflation wrong; I didn't see the current surge coming. But why? I d‚Ä¶\n",
      "https://t.co/o5e3mZLbDa\n",
      "Probably true. \n",
      "\n",
      "I know at least one CEO who spends a lot of time thinking about this‚Ä¶ https://t.co/jTqC2EMTaj\n",
      "https://t.co/zO1uBaYxUx\n",
      "https://t.co/n9NmawGxcN\n",
      "https://t.co/d6EZhbJ8i0\n",
      "https://t.co/iOhqfUK0DB\n",
      "Morning greetings from Jack London Park in Glen Ellen.  Spend two hours here and try and be in a bad mood. Go ahead‚Ä¶ https://t.co/kzg8yGaENl\n",
      "RT @IOHK_Charles: This is for you @lexfridman https://t.co/mZebnxQKBX\n"
     ]
    }
   ],
   "source": [
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter keyword or hastag to search: dogecoin\n",
      "Please enter how many tweets to analyze: 100\n"
     ]
    }
   ],
   "source": [
    "#Sentiment Analysis\n",
    "def percentage(part,whole):\n",
    " return 100 * float(part)/float(whole)\n",
    "keyword = input('Please enter keyword or hastag to search: ')\n",
    "noOfTweet = int(input ('Please enter how many tweets to analyze: '))\n",
    "tweets = tweepy.Cursor(api.search_tweets, q=keyword).items(noOfTweet)\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "tweet_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "for tweet in tweets:\n",
    " \n",
    " #print(tweet.text)\n",
    "    tweet_list.append(tweet.text)\n",
    "    analysis = TextBlob(tweet.text)\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    polarity += analysis.sentiment.polarity\n",
    "\n",
    "    if neg > pos:\n",
    "        negative_list.append(tweet.text)\n",
    "        negative += 1\n",
    "    elif pos > neg:\n",
    "        positive_list.append(tweet.text)\n",
    "        positive += 1\n",
    "    elif pos == neg:\n",
    "        neutral_list.append(tweet.text)\n",
    "        neutral += 1\n",
    "\n",
    "positive = percentage(positive, noOfTweet)\n",
    "negative = percentage(negative, noOfTweet)\n",
    "neutral = percentage(neutral, noOfTweet)\n",
    "polarity = percentage(polarity, noOfTweet)\n",
    "positive = format(positive, '.1f')\n",
    "negative = format(negative, '.1f')\n",
    "neutral = format(neutral, '.1f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number:  100\n",
      "positive number:  44\n",
      "negative number:  9\n",
      "neutral number:  47\n"
     ]
    }
   ],
   "source": [
    "#Number of Tweets (Total, Positive, Negative, Neutral)\n",
    "tweet_list = pd.DataFrame(tweet_list)\n",
    "neutral_list = pd.DataFrame(neutral_list)\n",
    "negative_list = pd.DataFrame(negative_list)\n",
    "positive_list = pd.DataFrame(positive_list)\n",
    "print('total number: ',len(tweet_list))\n",
    "print('positive number: ',len(positive_list))\n",
    "print('negative number: ', len(negative_list))\n",
    "print('neutral number: ',len(neutral_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating PieCart\n",
    "labels = ['Positive ['+str(positive)+'%]' , 'Neutral ['+str(neutral)+'%]','Negative ['+str(negative)+'%]']\n",
    "sizes = [positive, neutral, negative]\n",
    "colors = ['yellowgreen', 'blue‚Äô,‚Äôred‚Äô]\n",
    "patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
    "plt.style.use('default‚Äô)\n",
    "plt.legend(labels)\n",
    "plt.title(\"Sentiment Analysis Result for keyword= \"+keyword+\"\" )\n",
    "plt.axis('equal‚Äô)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Text (RT, Punctuation etc)\n",
    "#Creating new dataframe and new features\n",
    "tw_list = pd.DataFrame(tweet_list)\n",
    "tw_list[‚Äútext‚Äù] = tw_list[0]\n",
    "#Removing RT, Punctuation etc\n",
    "remove_rt = lambda x: re.sub(‚ÄòRT @\\w+: ‚Äò,‚Äù ‚Äú,x)\n",
    "rt = lambda x: re.sub(‚Äú(@[A-Za-z0‚Äì9]+)|([‚Å∞-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)‚Äù,‚Äù ‚Äú,x)\n",
    "tw_list[‚Äútext‚Äù] = tw_list.text.map(remove_rt).map(rt)\n",
    "tw_list[‚Äútext‚Äù] = tw_list.text.str.lower()\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Negative, Positive, Neutral and Compound values\n",
    "tw_list[['polarity', 'subjectivity']] = tw_list['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in tw_list['text'].iteritems():\n",
    " score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    " neg = score['neg']\n",
    " neu = score['neu']\n",
    " pos = score['pos']\n",
    " comp = score['compound']\n",
    " if neg > pos:\n",
    " tw_list.loc[index, 'sentiment'] = ‚Äúnegative‚Äù\n",
    " elif pos > neg:\n",
    " tw_list.loc[index, 'sentiment'] = ‚Äúpositive‚Äù\n",
    " else:\n",
    " tw_list.loc[index, 'sentiment'] = ‚Äúneutral‚Äù\n",
    " tw_list.loc[index, 'neg'] = neg\n",
    " tw_list.loc[index, 'neu'] = neu\n",
    " tw_list.loc[index, 'pos'] = pos\n",
    " tw_list.loc[index, 'compound'] = comp\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating new data frames for all sentiments (positive, negative and neutral)\n",
    "tw_list_negative = tw_list[tw_list[‚Äúsentiment‚Äù]==‚Äùnegative‚Äù]\n",
    "tw_list_positive = tw_list[tw_list[‚Äúsentiment‚Äù]==‚Äùpositive‚Äù]\n",
    "tw_list_neutral = tw_list[tw_list[‚Äúsentiment‚Äù]==‚Äùneutral‚Äù]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values_in_column(data,feature):\n",
    " total=data.loc[:,feature].value_counts(dropna=False)\n",
    " percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    " return pd.concat([total,percentage],axis=1,keys=[‚ÄòTotal‚Äô,‚ÄôPercentage‚Äô])\n",
    "#Count_values for sentiment\n",
    "count_values_in_column(tw_list,‚Äùsentiment‚Äù)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data for Pie Chart\n",
    "pichart = count_values_in_column(tw_list,‚Äùsentiment‚Äù)\n",
    "names= pc.index\n",
    "size=pc[‚ÄúPercentage‚Äù]\n",
    " \n",
    "# Create a circle for the center of the plot\n",
    "my_circle=plt.Circle( (0,0), 0.7, color=‚Äôwhite‚Äô)\n",
    "plt.pie(size, labels=names, colors=[‚Äògreen‚Äô,‚Äôblue‚Äô,‚Äôred‚Äô])\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Create Wordcloud\n",
    "def create_wordcloud(text):\n",
    " mask = np.array(Image.open(‚Äúcloud.png‚Äù))\n",
    " stopwords = set(STOPWORDS)\n",
    " wc = WordCloud(background_color=‚Äùwhite‚Äù,\n",
    " mask = mask,\n",
    " max_words=3000,\n",
    " stopwords=stopwords,\n",
    " repeat=True)\n",
    " wc.generate(str(text))\n",
    " wc.to_file(‚Äúwc.png‚Äù)\n",
    " print(‚ÄúWord Cloud Saved Successfully‚Äù)\n",
    " path=‚Äùwc.png‚Äù\n",
    " display(Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for all tweets\n",
    "create_wordcloud(tw_list[‚Äútext‚Äù].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for positive sentiment\n",
    "create_wordcloud(tw_list_positive[‚Äútext‚Äù].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for negative sentiment\n",
    "create_wordcloud(tw_list_negative[‚Äútext‚Äù].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating tweet‚Äôs lenght and word count\n",
    "tw_list[‚Äòtext_len‚Äô] = tw_list[‚Äòtext‚Äô].astype(str).apply(len)\n",
    "tw_list[‚Äòtext_word_count‚Äô] = tw_list[‚Äòtext‚Äô].apply(lambda x: len(str(x).split()))\n",
    "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_len.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(pd.DataFrame(tw_list.groupby(‚Äúsentiment‚Äù).text_word_count.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Punctuation\n",
    "def remove_punct(text):\n",
    " text = ‚Äú‚Äù.join([char for char in text if char not in string.punctuation])\n",
    " text = re.sub(‚Äò[0‚Äì9]+‚Äô, ‚Äò‚Äô, text)\n",
    " return text\n",
    "tw_list[‚Äòpunct‚Äô] = tw_list[‚Äòtext‚Äô].apply(lambda x: remove_punct(x))\n",
    "#Appliyng tokenization\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "tw_list['tokenized'] = tw_list['punct'].apply(lambda x: tokenization(x.lower()))\n",
    "#Removing stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "#Appliyng Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: stemming(x))\n",
    "#Cleaning Text\n",
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text\n",
    "tw_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appliyng Countvectorizer\n",
    "countVectorizer = CountVectorizer(analyzer=clean_text) \n",
    "countVector = countVectorizer.fit_transform(tw_list[‚Äòtext‚Äô])\n",
    "print(‚Äò{} Number of reviews has {} words‚Äô.format(countVector.shape[0], countVector.shape[1]))\n",
    "#print(countVectorizer.get_feature_names())\n",
    "1281 Number of reviews has 2966 words\n",
    "count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())\n",
    "count_vect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Used Words\n",
    "count = pd.DataFrame(count_vect_df.sum())\n",
    "countdf = count.sort_values(0,ascending=False).head(20)\n",
    "countdf[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to ngram\n",
    "def get_top_n_gram(corpus,ngram_range,n=None):\n",
    " vec = CountVectorizer(ngram_range=ngram_range,stop_words = ‚Äòenglish‚Äô).fit(corpus)\n",
    " bag_of_words = vec.transform(corpus)\n",
    " sum_words = bag_of_words.sum(axis=0) \n",
    " words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    " words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    " return words_freq[:n]\n",
    "#n2_bigram\n",
    "n2_bigrams = get_top_n_gram(tw_list[‚Äòtext‚Äô],(2,2),20)\n",
    "n2_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n3_trigram\n",
    "n3_trigrams = get_top_n_gram(tw_list[‚Äòtext‚Äô],(3,3),20)\n",
    "n3_trigrams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
